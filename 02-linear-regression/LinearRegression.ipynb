{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/master/02-linear-regression/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bJ2JqbrfLfj"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDvcL1FjsZLY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHoDZiwJsHlW"
      },
      "source": [
        "## y = wx + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8DvsrrgfPxC"
      },
      "source": [
        "Let's start with a toy 1D problem, where the true dependence is\n",
        "$$y=w\\cdot x+b+\\text{noise}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKRgT8hyxRm_"
      },
      "outputs": [],
      "source": [
        "def linear_function(x):\n",
        "  return 0.33 * x + 8.3\n",
        "\n",
        "def gen_dataset(N, func, lims=(-1., 1.), noise_lvl=0.2):\n",
        "  x = np.random.uniform(*lims, size=N)\n",
        "  y = func(x) + noise_lvl * np.random.normal(size=x.shape)\n",
        "  return x[:,None], y\n",
        "\n",
        "X, y = gen_dataset(50, linear_function)\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, linear_function(x))\n",
        "plt.scatter(X, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG5zspXRrYWL"
      },
      "source": [
        "### `LinearRegression` from `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92YYaRU9e2jz"
      },
      "outputs": [],
      "source": [
        "# The following class implements the analytical solution for\n",
        "# linear regression with the MSE loss\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, linear_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.legend()\n",
        "\n",
        "print(model.coef_, model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WGS--_XrSe9"
      },
      "source": [
        "### Sidenote: making contour plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndqMFPx6oTpP"
      },
      "outputs": [],
      "source": [
        "### Sidenote: making contour plots (level maps)\n",
        "\n",
        "plt.contourf(\n",
        "    [[0., 1., 2.], # matrix of X\n",
        "     [0., 1., 2.],\n",
        "     [0., 1., 2.]],\n",
        "    [[ 0.,  0.,  0.], # matrix of Y\n",
        "     [10., 10., 10.],\n",
        "     [20., 20., 20.]],\n",
        "    [[-1., 0., -1.], # matrix of Z\n",
        "     [ 0., 1.,  0.],\n",
        "     [-1., 0., -1.]]\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQs61KUCqV5G"
      },
      "outputs": [],
      "source": [
        "### 2D matrices of X and Y (as above) can be\n",
        "### created from 1D vectors using np.meshgrid:\n",
        "\n",
        "for i in np.meshgrid([0., 1., 2], [0., 10., 20.]):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-PAHAotrkJV"
      },
      "source": [
        "### MSE as a function of model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFQzedCIst18"
      },
      "source": [
        "Let's see what MSE looks like as a function of model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNT3O5soiWwb"
      },
      "outputs": [],
      "source": [
        "# Creating a grid of model parameter values:\n",
        "ww, bb = np.meshgrid(\n",
        "    np.linspace(-10., 10., 50),\n",
        "    np.linspace(-5., 15., 50)\n",
        ")\n",
        "\n",
        "# Calculating the map of MSE values on the grid defined above, i.e.\n",
        "# for each (w, b) in (ww, bb) calculate MSE for the model y = w * x + b.\n",
        "\n",
        "MSE_map = ((X[..., np.newaxis] * ww[np.newaxis,...] + bb[np.newaxis,...] - y[..., np.newaxis, np.newaxis]) ** 2).mean(axis=0)\n",
        "\n",
        "# Automatic checks\n",
        "assert MSE_map.shape == ww.shape\n",
        "for i in [0, -1]:\n",
        "  for j in [0, -1]:\n",
        "    assert np.isclose(\n",
        "        MSE_map[i, j],\n",
        "        ((ww[i, j] * X.ravel() + bb[i, j] - y)**2).mean()\n",
        "    ), f'assert failed for point {i, j}'\n",
        "\n",
        "# Plotting:\n",
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plt.colorbar(plt.contourf(ww, bb, MSE_map, levels=30))\n",
        "plt.scatter(model.coef_, model.intercept_, marker='*', s=150, c='orange');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_a_UsFsYPR"
      },
      "source": [
        "## Polynomial fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCSdUOg__u0n"
      },
      "source": [
        "Now let's take some arbitrary function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6X2splyeIVH"
      },
      "outputs": [],
      "source": [
        "def true_function(x):\n",
        "  return np.sin(3 * x + 0.8) + np.sin(1. / (x + 1.23))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPgBZq4l_0Ex"
      },
      "source": [
        "Obviously, we won't get a good fit with an ordinary linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmSBngOE0JdP"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "X, y = gen_dataset(25, true_function)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXhZyytaANX2"
      },
      "source": [
        "### `PolynomialFeatures` and pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbESfwRuAZD7"
      },
      "source": [
        "Even though our design matrix has only one column:\n",
        "$$X=\n",
        "\\begin{pmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_N\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "we can expand it with powers of $x$ to fit a polynomial:\n",
        "$$X'=\n",
        "\\begin{pmatrix}\n",
        "x_1 & (x_1)^2 & \\ldots & (x_1)^k \\\\\n",
        "x_2 & (x_2)^2 & \\ldots & (x_2)^k \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_N & (x_N)^2 & \\ldots & (x_N)^k\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "such that:\n",
        "\n",
        "$$\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2\\to \\underset{w}{\\text{min}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBX4E80aDfBv"
      },
      "source": [
        "This functionality is implemented in `sklearn.preprocessing.PolynomialFeatures`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tnvqh81sDeJb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_expand = PolynomialFeatures(3)\n",
        "poly_expand.fit_transform(np.arange(5)[:,None])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD4hq4nVE0WO"
      },
      "source": [
        "One can combine `PolynomialFeatures` (and any other transformers) along with the model into a single pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtNLILIP25-U"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXG8osw43CI-"
      },
      "outputs": [],
      "source": [
        "# The first parameter is the power of expansion. Try playing around with it.\n",
        "poly_expand = PolynomialFeatures(5, include_bias=False)\n",
        "linear_model = LinearRegression()\n",
        "model = make_pipeline(\n",
        "    poly_expand, linear_model\n",
        ")\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiWLsor3c2hp"
      },
      "source": [
        "Now we want to plot 2D projections of MSE as a function of model parameters **1 bonus point to HW**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTXh8fe9-gXE"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange, tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xZFirLY3hHj"
      },
      "outputs": [],
      "source": [
        "# Combine the weights and the bias into a single parameter vector\n",
        "solution = np.concatenate([linear_model.coef_, [linear_model.intercept_]])\n",
        "\n",
        "# Calculate the power expansion of the features\n",
        "X_expanded = np.concatenate([\n",
        "    poly_expand.transform(X), np.ones(shape=(len(X), 1))\n",
        "], axis=1)\n",
        "\n",
        "# We'll plot a large matrix of plots, so let's create\n",
        "# a 16 by 16 inch canvas\n",
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "# We'll loop over all pairs of weights\n",
        "i_img = 0\n",
        "for dim1 in trange(len(solution)):\n",
        "  for dim2 in range(len(solution)):\n",
        "    i_img += 1\n",
        "    # Skip the diagonal\n",
        "    if dim1 == dim2: continue\n",
        "\n",
        "    # Create the grid of parameter values\n",
        "    ww1, ww2 = np.meshgrid(\n",
        "        np.linspace(solution[dim1] - 1000., solution[dim1] + 1000., 50),\n",
        "        np.linspace(solution[dim2] - 1000., solution[dim2] + 1000., 50),\n",
        "    )\n",
        "\n",
        "    # Your turn! To calculate the map of MSE values, let's first\n",
        "    # create `param_grid` - a 3D array of parameter values of the\n",
        "    # following shape: (len(solution), ww1.shape[0], ww1.shape[1])\n",
        "    #\n",
        "    # I.e. `param_grid[i, :, :]` should equal to:\n",
        "    #     `ww1` if `i` equals `dim1`;\n",
        "    #     `ww2` if `i` equals `dim2`;\n",
        "    #     `solution[i]` otherwise.\n",
        "\n",
        "    param_grid = np.empty(shape= (len(solution),) + ww1.shape, dtype=float)\n",
        "    param_grid[:] = solution[:,None,None]\n",
        "    param_grid[dim1] = ww1\n",
        "    param_grid[dim2] = ww2\n",
        "\n",
        "    # Automatic checks\n",
        "    assert param_grid.shape == (len(solution),) + ww1.shape\n",
        "    assert np.allclose(param_grid[dim1], ww1)\n",
        "    assert np.allclose(param_grid[dim2], ww2)\n",
        "    assert all(\n",
        "        np.allclose(param_grid[i], solution[i])\n",
        "        for i in range(len(solution)) if i not in (dim1, dim2)\n",
        "    )\n",
        "\n",
        "    # Your turn! Now it's time to calculate the MSE map, i.e. for each grid\n",
        "    # element (i, j), you want `MSE_map[i, j]` to be equal to the MSE\n",
        "    # for the model defined by parameters `param_grid[:, i, j]`.\n",
        "    MSE_map = # <YOUR CODE>\n",
        "\n",
        "    # Automatic checks\n",
        "    assert MSE_map.shape == ww1.shape\n",
        "    for i in [0, -1]:\n",
        "      for j in [0, -1]:\n",
        "        assert np.isclose(\n",
        "            ((X_expanded @ param_grid[:, i, j] - y)**2).mean(),\n",
        "            MSE_map[i, j]\n",
        "        ), f'Check failed for point {i, j}'\n",
        "\n",
        "    plt.subplot(len(solution), len(solution), i_img)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.contourf(ww1, ww2, MSE_map, levels=10);\n",
        "    plt.scatter(solution[dim1], solution[dim2], marker='*', s=30, c='orange')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUbyj4uLMsMp"
      },
      "source": [
        "Note the relation between the amount of overfitting and correlation between parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Md8oRbHNJG"
      },
      "source": [
        "## Gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj-ZaSJCifkV"
      },
      "source": [
        "Let's look at MSE as a function of parameters:\n",
        "$$\\text{MSE}(w)=\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2$$\n",
        "\n",
        "Instead of minimizing it analytically, we can use numeric optimization with gradient descent. I.e. do the following procedure iteratively:\n",
        "$$w\\leftarrow w-\\alpha\\cdot\\frac{\\partial\\text{MSE}(w)}{\\partial w},$$\n",
        "for some constant *learning rate* $\\alpha$.\n",
        "\n",
        "For the task below you'll need to derive the analytical formula for $\\frac{\\partial\\text{MSE}(w)}{\\partial w}$. **Note, that $w$ is a vector!** If not sure how to do it, check out the [matrix calculus cheat sheet](https://en.wikipedia.org/wiki/Matrix_calculus#Identities).\n",
        "\n",
        "When done, play around with the power of the polynomial expansion, learning rate and the number of gradient descent steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AERh5jc-tBT"
      },
      "outputs": [],
      "source": [
        "X_expanded = np.concatenate([\n",
        "    poly_expand.transform(X), np.ones(shape=(len(X), 1))\n",
        "], axis=1)\n",
        "\n",
        "\n",
        "# Initialize the model parameters with zeros\n",
        "w = np.zeros(dtype=float, shape=X_expanded.shape[1])\n",
        "\n",
        "loss_values = [] # a list to keep track of how the loss value changes\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training loop\n",
        "for _ in trange(1000):\n",
        "  # Your turn: calculate the gradient of MSE with respect to w:\n",
        "  gradient = #<YOUR CODE>\n",
        "\n",
        "  # Automatic checks\n",
        "  assert gradient.shape == w.shape\n",
        "  assert (\n",
        "      ((X_expanded @ w - y)**2).mean() >\n",
        "      ((X_expanded @ (w - 1.e-6 * gradient) - y)**2).mean()\n",
        "  )\n",
        "\n",
        "  # Gradient descent step\n",
        "  w -= learning_rate * gradient\n",
        "\n",
        "  # Calculate and record the new loss value\n",
        "  loss_values.append(\n",
        "      ((X_expanded @ w - y)**2).mean()\n",
        "  )\n",
        "\n",
        "# Plotting the evolution of loss values\n",
        "plt.plot(loss_values);\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the solution\n",
        "x = np.linspace(-1, 1, 101)\n",
        "x_expanded = np.concatenate([\n",
        "    poly_expand.transform(x[:,None]),\n",
        "    np.ones(shape=(len(x), 1))\n",
        "], axis=1)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x,\n",
        "         x_expanded @ w, label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBenTP_0oDB-"
      },
      "source": [
        "Did you notice that numeric solution is less prone to overfitting? Some intuition for that can be found in this post: https://distill.pub/2017/momentum/ (though not explicitly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz42oYY9NK8X"
      },
      "source": [
        "## Here comes the overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-4VlHXJNK8X"
      },
      "source": [
        "What is going to happen if we increase the number of powers we use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb99I_IjNK8X"
      },
      "outputs": [],
      "source": [
        "X, y = gen_dataset(25, true_function)\n",
        "\n",
        "# Define the set of powers to be used generating the features\n",
        "set_of_powers =[ 1, 5, 15, 25 ]\n",
        "\n",
        "# Plotting the ground truth\n",
        "plt.figure(figsize=(15,6))\n",
        "x = np.linspace(-1, 1, 200)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y,marker='*', s = 100);\n",
        "plt.ylim(-2, 2)\n",
        "\n",
        "# Adding plots for every single value in the set of powers using the pipeline\n",
        "for d in set_of_powers:\n",
        "    poly_expand = PolynomialFeatures(d, include_bias=False)\n",
        "    model = make_pipeline(poly_expand, LinearRegression())\n",
        "    model.fit(X, y)\n",
        "    plt.plot(x, model.predict(x[:,None]),linewidth=2, label=\"$d=%d$\" % d)\n",
        "plt.legend(loc = 1)\n",
        "plt.title\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGm2N8J4NK8X"
      },
      "source": [
        "Using `sklearn` LinearRegression, it is possible to fit every single point, but it is not the objective we try to achieve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0no2-GlNK8X"
      },
      "source": [
        "## Bonus. Custom LinearRegression class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjyBFor8NK8Y"
      },
      "source": [
        "Based on [OML](https://github.com/girafe-ai/ml-mipt) open materials.\n",
        "\n",
        "Let's wrap our previous Gradient Descent experiments into LR class, using `sklearn` standard interfaces to implement it **(3 points)**.\n",
        "You can find [the base classes](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.base) for all of the most common problems.\n",
        "So, we need to inherit base class and implement main stages of regression pipeline methods:\n",
        "* hyperparameter initialization - constructor\n",
        "* parameters training on known objects - fit method\n",
        "* target estimation for unknown objects - predict method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvqft9LYNK8Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, RegressorMixin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkZYPBdXNK8Y"
      },
      "outputs": [],
      "source": [
        "class LinearRergessionSGD(BaseEstimator, RegressorMixin): # inherit base class\n",
        "    def __init__(self,\n",
        "                 batch_size: int=10,\n",
        "                 lr: float=1e-2,\n",
        "                 n_iters: int=10) -> None:\n",
        "        # You can read more about Annotations here: https://www.python.org/dev/peps/pep-3107/\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "\n",
        "        # We are going to use the history plotting the trajectory\n",
        "        self.w_history = []\n",
        "\n",
        "    def fit(self, X, Y):\n",
        "        # initialize the weights\n",
        "        w = <YOUR CODE>\n",
        "\n",
        "        n_objects = len(X)\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "            # Sample random indices of objects to be used during the step\n",
        "            sample_indices = <YOUR CODE>\n",
        "\n",
        "            # Make the step\n",
        "            w -= <YOUR CODE>\n",
        "            self.w_history.append(w.copy())\n",
        "\n",
        "        self.w = w\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Return the results of your model applied to X\n",
        "        return <YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jRQqs_eNK8Y"
      },
      "source": [
        "Now we can have a look on the weights' trajectory through the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCygLtmYNK8Y"
      },
      "outputs": [],
      "source": [
        "n_features = 2\n",
        "n_objects = 300\n",
        "batch_size = 10\n",
        "num_steps = 15\n",
        "np.random.seed(43)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM6w4qf0NK8Y"
      },
      "outputs": [],
      "source": [
        "# Let it be the *true* weights vector\n",
        "w_true = np.random.normal(size=(n_features))\n",
        "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
        "\n",
        "# Let's make different scales of features\n",
        "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n",
        "\n",
        "# Here you define the *true* target value\n",
        "Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b09fZAjsNK8Y"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y)\n",
        "\n",
        "# Fit the model of our custom class\n",
        "own_lr = LinearRergessionSGD(batch_size=batch_size, n_iters=num_steps).fit(x_train, y_train)\n",
        "# Get the weights\n",
        "w_list = np.array(own_lr.w_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h60gzh-kNK8Y"
      },
      "outputs": [],
      "source": [
        "# Compute level set\n",
        "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "\n",
        "levels = np.empty_like(A)\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        w_tmp = np.array([A[i, j], B[i, j]])\n",
        "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n",
        "\n",
        "plt.figure(figsize=(13, 9))\n",
        "plt.title('LR weights GD trajectory')\n",
        "plt.xlabel('$w_1$')\n",
        "plt.ylabel('$w_2$')\n",
        "plt.xlim(w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1)\n",
        "plt.ylim(w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1)\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "# Visualize the level set\n",
        "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1.8, num=10), cmap=plt.cm.rainbow_r)\n",
        "CB = plt.colorbar(CS, shrink=0.7)\n",
        "\n",
        "# Visualize the trajectory\n",
        "plt.scatter(w_true[0], w_true[1], c='r',marker = '*', s = 200)\n",
        "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
        "plt.plot(w_list[:, 0], w_list[:, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sfxjnVzNK8Z"
      },
      "source": [
        "## Here we go again..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efgHWPxfNK8Z"
      },
      "source": [
        "Now, instead of `sklearn` Linear Regression we use our own SGD-based impementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NFne5gRNK8Z"
      },
      "outputs": [],
      "source": [
        "X, y = gen_dataset(25, true_function)\n",
        "set_of_powers =[ 1, 5, 15, 25 ]\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "x = np.linspace(-1, 1, 200)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y,marker='*', s = 100);\n",
        "plt.ylim(-2, 2)\n",
        "\n",
        "for d in set_of_powers:\n",
        "    poly_expand = PolynomialFeatures(d, include_bias=True)\n",
        "    model = make_pipeline(poly_expand, LinearRergessionSGD(lr=1e-2,n_iters=2000))\n",
        "    model.fit(X, y)\n",
        "    plt.plot(x, model.predict(x[:,None]),linewidth=2, label=\"$d=%d$\" % d)\n",
        "\n",
        "plt.legend(loc = 1)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}